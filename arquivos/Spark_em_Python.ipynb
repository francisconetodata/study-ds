{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**O que é Apache Spark e por que é importante na área de dados?**\n",
        "\n",
        "Apache Spark é um *framework* de computação distribuída de código aberto, projetado para processamento e análise de grandes volumes de dados (*Big Data*).  A sua importância reside na sua capacidade de realizar estas tarefas de forma **rápida** e **escalável**, superando as limitações de sistemas tradicionais, especialmente quando lidamos com dados que não cabem na memória de uma única máquina.\n",
        "\n",
        "**Principais Características do Spark:**\n",
        "\n",
        "*   **Velocidade:**  Spark processa dados em memória (RAM), o que o torna significativamente mais rápido do que soluções baseadas em disco, como o Hadoop MapReduce para certas cargas de trabalho. Para operações iterativas e analíticas, o ganho de performance pode ser drástico.\n",
        "*   **Facilidade de Uso:** Spark oferece APIs de alto nível em várias linguagens (Python, Scala, Java, R), tornando-o acessível a um leque maior de desenvolvedores e cientistas de dados. A API do PySpark (para Python) é particularmente popular pela sua simplicidade e poder.\n",
        "*   **Generalidade:** Spark não é apenas para um tipo de tarefa. Ele oferece bibliotecas para SQL (Spark SQL), streaming (Spark Streaming e Structured Streaming), *machine learning* (MLlib) e processamento de grafos (GraphX), tudo dentro do mesmo *framework*.\n",
        "*   **Flexibilidade:**  Pode ser executado em diversas plataformas, desde *clusters* Hadoop, Kubernetes, *cloud* (AWS, Azure, GCP) até mesmo em *standalone mode* para desenvolvimento local.\n",
        "*   **Resiliência:** Spark é projetado para lidar com falhas. Utiliza o conceito de RDDs (Resilient Distributed Datasets) que explicaremos adiante, que garantem que os dados e as computações sejam tolerantes a falhas.\n",
        "\n",
        "**Arquitetura do Spark em Detalhe:**\n",
        "\n",
        "Para entender como o Spark funciona, é crucial compreender a sua arquitetura. Simplificadamente, um *cluster* Spark consiste em:\n",
        "\n",
        "*   **Driver Program:** É o processo principal onde sua aplicação Spark é executada. O *Driver* tem as seguintes responsabilidades:\n",
        "    *   **Cria o SparkContext (ou SparkSession):** Ponto de entrada para todas as funcionalidades do Spark.\n",
        "    *   **Gerencia o Ciclo de Vida da Aplicação:**  Coordena a execução das operações.\n",
        "    *   **Cria o DAG (Directed Acyclic Graph):**  Representa o plano de execução da aplicação, definindo as transformações e ações a serem realizadas nos dados.\n",
        "    *   **Agenda as Tarefas (Tasks):**  Divide o trabalho em tarefas e as distribui para os *Executors*.\n",
        "\n",
        "*   **Cluster Manager:**  É responsável por gerenciar os recursos do *cluster* (CPU, memória).  Pode ser:\n",
        "    *   **Standalone Spark Cluster Manager:**  Simples e fornecido pelo próprio Spark.\n",
        "    *   **Hadoop YARN (Yet Another Resource Negotiator):**  Utilizado em ambientes Hadoop.\n",
        "    *   **Apache Mesos:**  Outro *cluster manager* genérico.\n",
        "    *   **Kubernetes:**  Plataforma de orquestração de contêineres.\n",
        "\n",
        "*   **Worker Nodes:** Máquinas físicas ou virtuais que compõem o *cluster*. Cada *Worker Node* executa um ou mais:\n",
        "\n",
        "*   **Executors:** São processos que residem nos *Worker Nodes* e são responsáveis por:\n",
        "    *   **Executar as Tasks:**  Realizam as computações nos dados, de acordo com as instruções do *Driver*.\n",
        "    *   **Armazenar Dados em Cache:**  Mantêm os dados em memória para acesso rápido em operações futuras.\n",
        "    *   **Reportar Status para o Driver:**  Informam o *Driver* sobre o progresso e quaisquer erros.\n",
        "\n",
        "[Image of Spark Architecture]\n",
        "\n",
        "**Resilient Distributed Datasets (RDDs): A Base do Spark**\n",
        "\n",
        "RDDs são o conceito fundamental do Spark. Imagine um RDD como uma **coleção imutável e distribuída de objetos**.  As características chave dos RDDs são:\n",
        "\n",
        "*   **Resiliente:**  Se uma partição de um RDD se perder (por falha de um nó), o Spark consegue reconstruí-la automaticamente utilizando o *lineage* (histórico de operações) que levou à sua criação.\n",
        "*   **Distribuída:**  As partições de um RDD são espalhadas por diversos nós no *cluster*, permitindo o processamento paralelo.\n",
        "*   **Dataset:** Representam um conjunto de dados.\n",
        "*   **Imutável:**  Uma vez criado, um RDD não pode ser alterado. Operações em RDDs sempre criam novos RDDs.\n",
        "*   **Lazy Evaluation (Avaliação Preguiçosa):**  As transformações em RDDs não são executadas imediatamente. O Spark constrói o DAG de operações e só executa as computações quando uma **ação** (que retorna um resultado para o *Driver*) é chamada. Isso permite otimizar o plano de execução.\n",
        "\n",
        "**Operações em RDDs:**\n",
        "\n",
        "Existem dois tipos principais de operações em RDDs:\n",
        "\n",
        "1.  **Transformações:** Operações que criam um novo RDD a partir de um ou mais RDDs existentes. Exemplos:\n",
        "    *   `map()`: Aplica uma função a cada elemento do RDD.\n",
        "    *   `filter()`: Retorna um novo RDD com apenas os elementos que satisfazem uma condição.\n",
        "    *   `flatMap()`: Similar ao `map()`, mas pode retornar zero ou mais elementos para cada elemento de entrada.\n",
        "    *   `groupByKey()`: Agrupa os elementos por chave.\n",
        "    *   `reduceByKey()`: Agrupa por chave e reduz os valores de cada chave usando uma função.\n",
        "    *   `join()`: Junta dois RDDs baseados em uma chave comum.\n",
        "\n",
        "2.  **Ações:** Operações que computam um resultado e o retornam para o *Driver* ou escrevem em armazenamento externo.  A execução real das transformações agendadas ocorre quando uma ação é chamada. Exemplos:\n",
        "    *   `count()`: Retorna o número de elementos no RDD.\n",
        "    *   `collect()`: Retorna todos os elementos do RDD para o *Driver* (cuidado com RDDs grandes!).\n",
        "    *   `first()`: Retorna o primeiro elemento do RDD.\n",
        "    *   `take(n)`: Retorna os primeiros *n* elementos do RDD.\n",
        "    *   `reduce(func)`: Agrega os elementos do RDD usando uma função de redução.\n",
        "    *   `saveAsTextFile(path)`: Escreve o RDD em arquivos de texto em um sistema de arquivos (local, HDFS, etc.).\n",
        "\n",
        "**DataFrames e Datasets: Evolução e Facilidade de Uso**\n",
        "\n",
        "Embora os RDDs sejam a base do Spark, para muitas tarefas de análise de dados, especialmente aquelas que envolvem dados estruturados ou semiestruturados, o Spark SQL introduziu **DataFrames** e **Datasets**.\n",
        "\n",
        "*   **DataFrames:** São como tabelas em bancos de dados relacionais, com dados organizados em colunas nomeadas.  Oferecem:\n",
        "    *   **Schema:** Dados em DataFrames têm um esquema definido (nomes e tipos de dados das colunas), o que permite ao Spark otimizar a execução.\n",
        "    *   **Otimizações do Spark SQL:**  Spark SQL usa um otimizador chamado Catalyst para melhorar o plano de execução das consultas em DataFrames. Ele também aproveita o motor Tungsten para otimizar o uso de memória e CPU.\n",
        "    *   **APIs Ricas:**  Oferecem APIs convenientes para consulta e manipulação de dados, similares a SQL e às APIs de manipulação de DataFrames em linguagens como Pandas.\n",
        "\n",
        "*   **Datasets:**  São uma extensão dos DataFrames, oferecendo:\n",
        "    *   **Tipagem Forte:** Além do esquema, Datasets são fortemente tipados em linguagens como Scala e Java. Em Python, a tipagem forte é menos evidente, mas ainda há benefícios em termos de otimização e segurança de tipo em tempo de compilação (em Scala/Java).\n",
        "    *   **Flexibilidade:**  Combinação das vantagens de DataFrames com a segurança de tipo e capacidades de programação orientada a objetos de RDDs.\n",
        "\n",
        "**Em resumo, DataFrames e Datasets são construções de mais alto nível que facilitam muito o trabalho com dados estruturados no Spark e geralmente oferecem melhor performance do que trabalhar diretamente com RDDs para este tipo de dado.** Para a maioria das tarefas de análise de dados, DataFrames são a escolha recomendada.\n",
        "\n",
        "**Exemplos Práticos em Python com PySpark**\n",
        "\n",
        "Vamos agora criar exemplos práticos usando PySpark para ilustrar os conceitos que discutimos. Primeiro, certifique-se de ter o PySpark instalado. Se não, você pode instalá-lo usando pip:\n",
        "\n",
        "```bash\n",
        "pip install pyspark\n",
        "```\n",
        "\n",
        "E também pode precisar do `findspark` para facilitar a inicialização do Spark no modo local (para desenvolvimento):\n",
        "\n",
        "```bash\n",
        "pip install findspark\n",
        "```\n",
        "\n",
        "**Exemplo 1: Contagem de Palavras em um Arquivo de Texto**\n",
        "\n",
        "Este é um exemplo clássico para demonstrar operações básicas do Spark."
      ],
      "metadata": {
        "id": "l31_wSyHxkH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Inicializa a SparkSession (ponto de entrada para DataFrames e APIs SQL)\n",
        "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
        "\n",
        "# Caminho para o arquivo de texto (substitua pelo seu arquivo)\n",
        "file_path = \"caminho/para/seu/arquivo.txt\"\n",
        "\n",
        "# Lê o arquivo de texto como um RDD de strings\n",
        "lines = spark.sparkContext.textFile(file_path)\n",
        "\n",
        "# 1. Quebra cada linha em palavras (flatMap)\n",
        "words = lines.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "# 2. Mapeia cada palavra para um par (palavra, 1) (map)\n",
        "word_pairs = words.map(lambda word: (word, 1))\n",
        "\n",
        "# 3. Reduz por chave (palavra), somando as contagens (reduceByKey)\n",
        "word_counts = word_pairs.reduceByKey(lambda count1, count2: count1 + count2)\n",
        "\n",
        "# 4. Coleta os resultados e imprime\n",
        "output = word_counts.collect()\n",
        "for (word, count) in output:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# Encerra a SparkSession\n",
        "spark.stop()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ew_9TC4ExkH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicação Passo a Passo:**\n",
        "\n",
        "1.  **`SparkSession.builder.appName(\"WordCount\").getOrCreate()`:** Inicializa a `SparkSession`. `appName` define o nome da sua aplicação Spark, útil para monitoramento na interface do Spark. `getOrCreate()` cria uma nova `SparkSession` se não existir uma, ou retorna uma existente.\n",
        "\n",
        "2.  **`spark.sparkContext.textFile(file_path)`:** Usa o `SparkContext` (acessível através de `spark.sparkContext`) para ler um arquivo de texto.  `textFile()` retorna um RDD onde cada elemento é uma linha do arquivo.\n",
        "\n",
        "3.  **`lines.flatMap(lambda line: line.split(\" \"))`:**\n",
        "    *   `flatMap()` é uma transformação.\n",
        "    *   A função lambda `lambda line: line.split(\" \")` é aplicada a cada linha do RDD `lines`.\n",
        "    *   `line.split(\" \")` quebra a linha em uma lista de palavras, usando o espaço como delimitador.\n",
        "    *   `flatMap()` \"achata\" as listas de palavras resultantes em um único RDD `words`, onde cada elemento é uma palavra individual.\n",
        "\n",
        "4.  **`words.map(lambda word: (word, 1))`:**\n",
        "    *   `map()` é outra transformação.\n",
        "    *   A função lambda `lambda word: (word, 1)` mapeia cada palavra para um par chave-valor, onde a palavra é a chave e o valor é 1 (representando uma ocorrência da palavra).\n",
        "\n",
        "5.  **`word_pairs.reduceByKey(lambda count1, count2: count1 + count2)`:**\n",
        "    *   `reduceByKey()` é uma transformação que opera em RDDs de pares chave-valor.\n",
        "    *   Agrupa todos os pares com a mesma chave (palavra).\n",
        "    *   A função lambda `lambda count1, count2: count1 + count2` é usada para \"reduzir\" os valores para cada chave. Neste caso, ela soma as contagens (1s) para cada palavra, resultando na contagem total de cada palavra.\n",
        "\n",
        "6.  **`word_counts.collect()`:**\n",
        "    *   `collect()` é uma **ação**. Ela traz **todos** os elementos do RDD `word_counts` para o *Driver Program* (memória da máquina onde o programa Python está rodando). **Cuidado:**  Não use `collect()` em RDDs muito grandes, pois pode causar estouro de memória no Driver.\n",
        "    *   Neste caso, como esperamos que a contagem de palavras não seja massiva, `collect()` é aceitável para fins de demonstração.\n",
        "\n",
        "7.  **Loop para imprimir os resultados:**  Itera sobre a lista de pares (palavra, contagem) retornada por `collect()` e imprime cada palavra e sua contagem.\n",
        "\n",
        "8.  **`spark.stop()`:** Encerra a `SparkSession`, liberando recursos.\n",
        "\n",
        "**Exemplo 2: Análise de Dados com DataFrame (Leitura de CSV, Filtro e Agregação)**\n",
        "\n",
        "Vamos usar DataFrames para um exemplo mais estruturado. Suponha que você tenha um arquivo CSV com dados de vendas:\n",
        "\n",
        "`vendas.csv`:\n",
        "\n",
        "```csv\n",
        "produto,categoria,preco,quantidade,data_venda\n",
        "ProdutoA,Eletrônicos,100,2,2023-10-26\n",
        "ProdutoB,Roupas,50,5,2023-10-26\n",
        "ProdutoC,Eletrônicos,200,1,2023-10-27\n",
        "ProdutoA,Eletrônicos,100,3,2023-10-27\n",
        "ProdutoD,Livros,25,10,2023-10-28\n",
        "ProdutoB,Roupas,50,2,2023-10-28\n",
        "```"
      ],
      "metadata": {
        "id": "XxV0occ2xkH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, max\n",
        "\n",
        "# Inicializa SparkSession\n",
        "spark = SparkSession.builder.appName(\"VendasAnalysis\").getOrCreate()\n",
        "\n",
        "# Caminho para o arquivo CSV\n",
        "csv_path = \"caminho/para/seu/vendas.csv\"\n",
        "\n",
        "# Lê o arquivo CSV para um DataFrame (inferindo o esquema)\n",
        "vendas_df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
        "\n",
        "# Imprime o esquema do DataFrame\n",
        "vendas_df.printSchema()\n",
        "\n",
        "# Mostra as primeiras linhas do DataFrame\n",
        "vendas_df.show()\n",
        "\n",
        "# 1. Filtra vendas de produtos eletrônicos\n",
        "eletronicos_df = vendas_df.filter(col(\"categoria\") == \"Eletrônicos\")\n",
        "eletronicos_df.show()\n",
        "\n",
        "# 2. Agrupa por categoria e calcula o preço médio e o preço máximo\n",
        "agregado_df = vendas_df.groupBy(\"categoria\").agg(\n",
        "    avg(\"preco\").alias(\"preco_medio\"),\n",
        "    max(\"preco\").alias(\"preco_maximo\")\n",
        ")\n",
        "agregado_df.show()\n",
        "\n",
        "# Encerra SparkSession\n",
        "spark.stop()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "9b_lXEOhxkH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicação Passo a Passo:**\n",
        "\n",
        "1.  **`spark.read.csv(...)`:**  Usa `spark.read` para ler um arquivo CSV e criar um DataFrame.\n",
        "    *   `csv_path`: Caminho para o arquivo CSV.\n",
        "    *   `header=True`: Indica que a primeira linha do CSV contém os cabeçalhos das colunas.\n",
        "    *   `inferSchema=True`:  Pede ao Spark para tentar inferir automaticamente o tipo de dados de cada coluna (por exemplo, string, integer, double).\n",
        "\n",
        "2.  **`vendas_df.printSchema()`:**  Imprime o esquema inferido do DataFrame, mostrando os nomes das colunas e seus tipos de dados. Isso é útil para verificar se o Spark interpretou os dados corretamente.\n",
        "\n",
        "3.  **`vendas_df.show()`:**  Mostra as primeiras 20 linhas do DataFrame na saída do console. Útil para visualizar os dados.\n",
        "\n",
        "4.  **`vendas_df.filter(col(\"categoria\") == \"Eletrônicos\")`:**\n",
        "    *   `filter()`: Filtra as linhas do DataFrame com base em uma condição.\n",
        "    *   `col(\"categoria\") == \"Eletrônicos\"`:  Define a condição. `col(\"categoria\")` seleciona a coluna \"categoria\" do DataFrame.  `== \"Eletrônicos\"` compara o valor da coluna com a string \"Eletrônicos\".\n",
        "\n",
        "5.  **`vendas_df.groupBy(\"categoria\").agg(...)`:**\n",
        "    *   `groupBy(\"categoria\")`: Agrupa as linhas do DataFrame pela coluna \"categoria\".\n",
        "    *   `.agg(...)`: Aplica funções de agregação aos grupos.\n",
        "        *   `avg(\"preco\").alias(\"preco_medio\")`: Calcula a média da coluna \"preco\" para cada grupo e renomeia a coluna resultante para \"preco_medio\".\n",
        "        *   `max(\"preco\").alias(\"preco_maximo\")`: Calcula o valor máximo da coluna \"preco\" para cada grupo e renomeia para \"preco_maximo\".\n",
        "\n",
        "6.  **`agregado_df.show()`:**  Mostra o DataFrame resultante da agregação, que contém as categorias e o preço médio e máximo para cada categoria.\n",
        "\n",
        "**Streaming de Dados com Spark (Spark Streaming e Structured Streaming)**\n",
        "\n",
        "Spark também oferece capacidades poderosas para processamento de *streaming* de dados, ou seja, dados que chegam continuamente em tempo real. Existem duas APIs principais para *streaming* no Spark:\n",
        "\n",
        "*   **Spark Streaming (DStreams):**  A API original de *streaming* do Spark.  Baseia-se no conceito de **DStreams (Discretized Streams)**. Um DStream é uma sequência de RDDs, onde cada RDD representa um lote de dados que chegou em um determinado intervalo de tempo (o *batch interval*).\n",
        "\n",
        "    *   **Funcionamento:** Spark Streaming recebe fluxos de dados de várias fontes (Kafka, Flume, TCP sockets, etc.), divide o fluxo em pequenos lotes (batches) e processa cada lote usando operações em RDDs. Os resultados dos processamentos em lote são então combinados para produzir o fluxo de saída.\n",
        "\n",
        "    *   **Exemplo Simplificado (Word Count em Streaming de um Socket):**"
      ],
      "metadata": {
        "id": "d_KNrlwFxkH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "        from pyspark.streaming import StreamingContext\n",
        "\n",
        "        # Inicializa SparkContext\n",
        "        sc = SparkContext(appName=\"StreamingWordCount\")\n",
        "        # Cria StreamingContext com batch interval de 1 segundo\n",
        "        ssc = StreamingContext(sc, 1)\n",
        "\n",
        "        # Cria DStream a partir de um socket (ex: netcat)\n",
        "        lines = ssc.socketTextStream(\"localhost\", 9999)\n",
        "\n",
        "        # Processamento (similar ao Word Count de arquivo):\n",
        "        words = lines.flatMap(lambda line: line.split(\" \"))\n",
        "        pairs = words.map(lambda word: (word, 1))\n",
        "        word_counts = pairs.reduceByKey(lambda count1, count2: count1 + count2)\n",
        "\n",
        "        # Imprime os resultados a cada lote\n",
        "        word_counts.pprint()\n",
        "\n",
        "        # Inicia o streaming\n",
        "        ssc.start()\n",
        "        ssc.awaitTermination()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "iy1D63KNxkIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Para testar:**\n",
        "        1.  Execute este script Python.\n",
        "        2.  Abra um terminal e use o comando `netcat` (ou `nc`) para enviar dados para `localhost:9999`: `nc -lk 9999`\n",
        "        3.  Digite frases no terminal do `netcat` e observe as contagens de palavras sendo impressas no console do script Spark Streaming.\n",
        "\n",
        "    *   **Observações sobre Spark Streaming (DStreams):**\n",
        "        *   **Micro-Batching:**  Spark Streaming usa micro-batching, o que significa que o *streaming* é simulado processando os dados em pequenos lotes. Isso pode introduzir alguma latência (atraso) inerente ao tamanho do *batch interval*.\n",
        "        *   **Menos Integração com DataFrame/Dataset API:**  Embora seja possível converter DStreams para RDDs e então para DataFrames, a integração não é tão direta e fluida como na Structured Streaming.\n",
        "\n",
        "*   **Structured Streaming:** Uma API mais recente e recomendada para *streaming* no Spark. Construída sobre o Spark SQL engine, oferece:\n",
        "\n",
        "    *   **API Baseada em DataFrames/Datasets:**  Permite usar a mesma API de DataFrames e Datasets para processar *streaming* de dados, o que torna a programação mais intuitiva e unificada com o processamento *batch*.\n",
        "    *   **Processamento Contínuo (Continuous Processing):** Além do micro-batching, Structured Streaming também suporta processamento contínuo (em algumas configurações e fontes de dados), que pode reduzir a latência.\n",
        "    *   **Maior Tolerância a Falhas e Garantias de Consistência:**  Structured Streaming oferece garantias mais fortes de tolerância a falhas e consistência de resultados.\n",
        "    *   **Suporte a Janelas de Tempo (Windowing):**  Facilita a realização de agregações e análises em janelas de tempo (por exemplo, contagem de eventos nos últimos 5 minutos, média móvel, etc.).\n",
        "\n",
        "    *   **Exemplo Simplificado (Word Count em Structured Streaming de um Socket):**"
      ],
      "metadata": {
        "id": "mzKUZzJtxkIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "        from pyspark.sql.functions import explode, split, window, count\n",
        "\n",
        "        # Inicializa SparkSession\n",
        "        spark = SparkSession.builder.appName(\"StructuredStreamingWordCount\").getOrCreate()\n",
        "\n",
        "        # Cria DataFrame de streaming a partir de um socket\n",
        "        lines_df = spark.readStream \\\n",
        "            .format(\"socket\") \\\n",
        "            .option(\"host\", \"localhost\") \\\n",
        "            .option(\"port\", 9999) \\\n",
        "            .load()\n",
        "\n",
        "        # Divide as linhas em palavras\n",
        "        words_df = lines_df.select(explode(split(lines_df.value, \" \")).alias(\"word\"))\n",
        "\n",
        "        # Agrega por palavra e janela de tempo (janela de 10 segundos, slide de 5 segundos)\n",
        "        word_counts_df = words_df.groupBy(\n",
        "            window(words_df.timestamp, \"10 seconds\", \"5 seconds\"),\n",
        "            words_df.word\n",
        "        ).agg(count(\"*\").alias(\"count\"))\n",
        "\n",
        "        # Inicia a query de streaming e imprime no console\n",
        "        query = word_counts_df.writeStream \\\n",
        "            .outputMode(\"complete\") \\\n",
        "            .format(\"console\") \\\n",
        "            .start()\n",
        "\n",
        "        query.awaitTermination()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "EJFrV2twxkIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Para testar:** Similar ao exemplo anterior de Spark Streaming, execute este script e use `netcat` para enviar dados para `localhost:9999`. Observe que, neste caso, as contagens de palavras são agrupadas por janelas de tempo.\n",
        "\n",
        "    *   **Observações sobre Structured Streaming:**\n",
        "        *   **Recomendado para Novas Aplicações:**  Structured Streaming é geralmente a API recomendada para novas aplicações de *streaming* no Spark, devido à sua API mais moderna, melhor integração com DataFrames/Datasets e recursos avançados.\n",
        "        *   **Maior Facilidade para Operações Complexas:** Facilita a realização de operações mais complexas em *streaming*, como joins entre streams e dados *batch*, operações de janela, etc.\n",
        "\n",
        "**Conclusão:**\n",
        "\n",
        "Apache Spark é uma ferramenta essencial no ecossistema de *Big Data* para processamento rápido e escalável de grandes volumes de dados. Seja para processamento *batch* (com RDDs, DataFrames) ou *streaming* (com Spark Streaming ou Structured Streaming), o Spark oferece um conjunto de APIs e funcionalidades versáteis para diversas tarefas na área de dados, desde análise exploratória e limpeza de dados até *machine learning* e processamento em tempo real.  A escolha entre RDDs, DataFrames/Datasets e entre Spark Streaming e Structured Streaming dependerá dos requisitos específicos do seu projeto e do tipo de dados com que você está trabalhando."
      ],
      "metadata": {
        "id": "eyY6eDeBxkIA"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}