{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Vamos explorar como consultar bases de dados SQL Server utilizando tanto SQL puro como Python, focando no uso do Spark (PySpark) e mencionando outras ferramentas relevantes.  O objetivo é fornecer um guia completo, desde os conceitos básicos até exemplos práticos, incluindo a criação de uma métrica calculada periodicamente.\n",
        "\n",
        "**Parte 1: Consulta SQL Server com SQL (Diretamente)**\n",
        "\n",
        "Para começar, é fundamental entender como consultar o SQL Server utilizando a linguagem SQL padrão. Esta é a base para qualquer interação, mesmo quando utilizamos ferramentas como Python e Spark.\n",
        "\n",
        "**Conceitos SQL Essenciais para Consulta:**\n",
        "\n",
        "*   **`SELECT`:**  A instrução fundamental para recuperar dados. Especifica as colunas que deseja retornar.\n",
        "\n",
        "    ```sql\n",
        "    SELECT coluna1, coluna2, ...\n",
        "    FROM nome_da_tabela;\n",
        "    ```\n",
        "\n",
        "*   **`FROM`:** Indica a tabela da qual os dados serão selecionados.\n",
        "\n",
        "*   **`WHERE`:** Filtra as linhas com base em uma ou mais condições.\n",
        "\n",
        "    ```sql\n",
        "    SELECT coluna1, coluna2\n",
        "    FROM nome_da_tabela\n",
        "    WHERE condição;\n",
        "    ```\n",
        "\n",
        "*   **Operadores de Comparação:**  Utilizados no `WHERE` para definir condições (`=`, `>`, `<`, `>=`, `<=`, `<>`, `!=`, `LIKE`, `IN`, `BETWEEN`, `IS NULL`, `IS NOT NULL`).\n",
        "\n",
        "*   **Operadores Lógicos:**  Combinam condições (`AND`, `OR`, `NOT`).\n",
        "\n",
        "*   **`JOIN`:** Combina dados de duas ou mais tabelas relacionadas.\n",
        "\n",
        "    *   `INNER JOIN`: Retorna apenas linhas correspondentes em ambas as tabelas.\n",
        "    *   `LEFT JOIN`: Retorna todas as linhas da tabela da esquerda e as correspondências da tabela da direita (se houver).\n",
        "    *   `RIGHT JOIN`: Retorna todas as linhas da tabela da direita e as correspondências da tabela da esquerda (se houver).\n",
        "    *   `FULL OUTER JOIN`: Retorna todas as linhas de ambas as tabelas, com correspondências sempre que possível.\n",
        "\n",
        "    ```sql\n",
        "    SELECT tabela1.coluna1, tabela2.coluna2\n",
        "    FROM tabela1\n",
        "    INNER JOIN tabela2 ON tabela1.coluna_chave = tabela2.coluna_chave;\n",
        "    ```\n",
        "\n",
        "*   **Funções de Agregação:** Realizam cálculos em conjuntos de dados (`COUNT`, `SUM`, `AVG`, `MIN`, `MAX`).\n",
        "\n",
        "    ```sql\n",
        "    SELECT COUNT(*), AVG(coluna_valor)\n",
        "    FROM nome_da_tabela\n",
        "    WHERE condição;\n",
        "    ```\n",
        "\n",
        "*   **`GROUP BY`:** Agrupa linhas com valores semelhantes em uma ou mais colunas, frequentemente usado com funções de agregação.\n",
        "\n",
        "    ```sql\n",
        "    SELECT coluna_agrupamento, COUNT(*)\n",
        "    FROM nome_da_tabela\n",
        "    GROUP BY coluna_agrupamento;\n",
        "    ```\n",
        "\n",
        "*   **`ORDER BY`:** Ordena os resultados por uma ou mais colunas (`ASC` para ascendente, `DESC` para descendente).\n",
        "\n",
        "    ```sql\n",
        "    SELECT coluna1, coluna2\n",
        "    FROM nome_da_tabela\n",
        "    ORDER BY coluna1 ASC, coluna2 DESC;\n",
        "    ```\n",
        "\n",
        "**Exemplo SQL Server Básico:**\n",
        "\n",
        "Suponha que temos uma tabela `Vendas` no SQL Server com as colunas: `ID_Venda`, `Produto`, `Data_Venda`, `Quantidade`, `Preco_Unitario`.\n",
        "\n",
        "**Consultas de exemplo:**\n",
        "\n",
        "1.  **Selecionar todas as colunas e linhas:**\n",
        "\n",
        "    ```sql\n",
        "    SELECT *\n",
        "    FROM Vendas;\n",
        "    ```\n",
        "\n",
        "2.  **Selecionar apenas `Produto` e `Data_Venda` para vendas após '2023-01-01':**\n",
        "\n",
        "    ```sql\n",
        "    SELECT Produto, Data_Venda\n",
        "    FROM Vendas\n",
        "    WHERE Data_Venda > '2023-01-01';\n",
        "    ```\n",
        "\n",
        "3.  **Calcular o total de vendas por produto:**\n",
        "\n",
        "    ```sql\n",
        "    SELECT Produto, SUM(Quantidade * Preco_Unitario) AS Total_Vendas\n",
        "    FROM Vendas\n",
        "    GROUP BY Produto\n",
        "    ORDER BY Total_Vendas DESC;\n",
        "    ```\n",
        "\n",
        "**Parte 2: Consulta SQL Server com Python e PySpark**\n",
        "\n",
        "PySpark é uma excelente ferramenta para trabalhar com SQL Server a partir do Python, especialmente quando lidamos com grandes volumes de dados e necessitamos de processamento distribuído e escalabilidade.\n",
        "\n",
        "**Pré-requisitos PySpark para SQL Server:**\n",
        "\n",
        "1.  **Instalar PySpark:** Se ainda não instalou, utilize `pip install pyspark`.\n",
        "2.  **JDBC Driver para SQL Server:** PySpark utiliza JDBC para se conectar a bancos de dados relacionais como o SQL Server. Você precisará do driver JDBC para SQL Server. Pode baixar o driver da Microsoft e certificar-se de que o caminho para o arquivo JAR do driver esteja acessível ao PySpark (normalmente colocando-o na pasta `jars` do Spark ou especificando no classpath).\n",
        "\n",
        "**Conectando ao SQL Server com PySpark:**\n",
        "\n",
        "Utilizamos a função `spark.read.jdbc()` para ler dados do SQL Server para um DataFrame do PySpark."
      ],
      "metadata": {
        "id": "lEhOw7xpzR6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Inicializar SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SQLServerConsulta\") \\\n",
        "    .config(\"spark.jars\", \"caminho/para/sqljdbc_auth.jar,caminho/para/mssql-jdbc-12.4.0.jre11.jar\") \\ # Caminho para os seus drivers JDBC\n",
        "    .getOrCreate()\n",
        "\n",
        "# Configurações de conexão JDBC\n",
        "jdbcUrl = \"jdbc:sqlserver://servidorSQLServer:1433;databaseName=nomeDoBancoDeDados;integratedSecurity=true;\" # Para autenticação integrada do Windows\n",
        "# jdbcUrl = \"jdbc:sqlserver://servidorSQLServer:1433;databaseName=nomeDoBancoDeDados;\" # Para autenticação SQL Server\n",
        "connectionProperties = {\n",
        "    \"user\": \"seu_usuario_sqlserver\", # Se usar autenticação SQL Server\n",
        "    \"password\": \"sua_senha_sqlserver\", # Se usar autenticação SQL Server\n",
        "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
        "}\n",
        "table_name = \"nome_da_tabela_sqlserver\"\n",
        "\n",
        "# Ler dados do SQL Server para um DataFrame\n",
        "df_vendas = spark.read.jdbc(url=jdbcUrl, table=table_name, properties=connectionProperties)\n",
        "\n",
        "# Mostrar o esquema do DataFrame e as primeiras linhas\n",
        "df_vendas.printSchema()\n",
        "df_vendas.show()\n",
        "\n",
        "# Encerra a SparkSession\n",
        "spark.stop()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "zk2HXHkuzR6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicação do código:**\n",
        "\n",
        "*   **`SparkSession.builder...config(\"spark.jars\", ...).getOrCreate()`:** Inicializa a `SparkSession` e configura o caminho para os arquivos JAR dos drivers JDBC.  É crucial incluir os JARs corretos para que o PySpark consiga se comunicar com o SQL Server.  Você pode precisar ajustar os caminhos e os nomes dos arquivos JAR dependendo da sua versão do driver.\n",
        "*   **`jdbcUrl`:** A URL de conexão JDBC para o seu SQL Server. Ajuste:\n",
        "    *   `servidorSQLServer`:  Nome ou endereço IP do seu servidor SQL Server.\n",
        "    *   `1433`: Porta padrão do SQL Server (altere se for diferente).\n",
        "    *   `databaseName=nomeDoBancoDeDados`: Nome do banco de dados no SQL Server que deseja consultar.\n",
        "    *   `integratedSecurity=true;`: Usado para autenticação integrada do Windows (o usuário que executa o Python precisa ter permissão no SQL Server). Se usar autenticação SQL Server, comente esta parte e utilize `user` e `password` em `connectionProperties`.\n",
        "*   **`connectionProperties`:** Dicionário com propriedades de conexão.  Importante para fornecer credenciais se usar autenticação SQL Server (`user`, `password`) e especificar o driver JDBC (`driver`).\n",
        "*   **`table_name`:** Nome da tabela no SQL Server que você deseja ler.\n",
        "*   **`spark.read.jdbc(...)`:**  Função que lê dados via JDBC e cria um DataFrame.\n",
        "    *   `url`: A URL de conexão JDBC.\n",
        "    *   `table`: O nome da tabela ou uma subconsulta SQL (ver exemplo abaixo).\n",
        "    *   `properties`: As propriedades de conexão.\n",
        "*   **`df_vendas.printSchema()` e `df_vendas.show()`:** Funções para visualizar o esquema do DataFrame (tipos de dados das colunas) e as primeiras linhas de dados.\n",
        "*   **`spark.stop()`:** Encerra a sessão Spark.\n",
        "\n",
        "**Consultas SQL Diretas com `spark.sql()`:**\n",
        "\n",
        "Você também pode executar consultas SQL diretamente no SQL Server através do PySpark utilizando `spark.sql()`, mas primeiro precisa registrar a tabela JDBC como uma tabela temporária no Spark."
      ],
      "metadata": {
        "id": "_hiQHZa5zR6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (código de inicialização SparkSession e configurações JDBC como no exemplo anterior) ...\n",
        "\n",
        "# Registrar tabela JDBC como uma view temporária\n",
        "df_vendas.createOrReplaceTempView(\"vendas_temp_view\")\n",
        "\n",
        "# Executar consulta SQL diretamente usando spark.sql()\n",
        "df_resultado_sql = spark.sql(\"\"\"\n",
        "    SELECT Produto, SUM(Quantidade * Preco_Unitario) AS Total_Vendas\n",
        "    FROM vendas_temp_view\n",
        "    GROUP BY Produto\n",
        "    ORDER BY Total_Vendas DESC\n",
        "\"\"\")\n",
        "\n",
        "df_resultado_sql.show()\n",
        "\n",
        "# ... (spark.stop()) ..."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "4vnIgp9rzR6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utilizando subconsultas SQL no `spark.read.jdbc()`:**\n",
        "\n",
        "Em vez de ler uma tabela inteira, você pode usar uma subconsulta SQL diretamente no `spark.read.jdbc()` para ler apenas os dados que precisa."
      ],
      "metadata": {
        "id": "E8JLylDazR6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (código de inicialização SparkSession e configurações JDBC) ...\n",
        "\n",
        "subquery = \"(SELECT Produto, Data_Venda, Quantidade, Preco_Unitario FROM Vendas WHERE Data_Venda > '2023-01-01') AS vendas_filtradas\"\n",
        "\n",
        "df_vendas_filtradas = spark.read.jdbc(url=jdbcUrl, table=subquery, properties=connectionProperties)\n",
        "\n",
        "df_vendas_filtradas.show()\n",
        "\n",
        "# ... (spark.stop()) ..."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "TPoZtaYBzR6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parte 3: Exemplo Prático - Cálculo Periódico de Métricas**\n",
        "\n",
        "Vamos criar um exemplo que consulta o SQL Server periodicamente para calcular uma nova métrica: **o total de vendas do dia anterior**.\n",
        "\n",
        "**Cenário:**\n",
        "\n",
        "Queremos monitorar diariamente o total de vendas realizadas no dia anterior. Para isso, vamos criar um script Python que:\n",
        "\n",
        "1.  Conecta-se ao SQL Server.\n",
        "2.  Executa uma consulta para somar as vendas do dia anterior.\n",
        "3.  Imprime a métrica calculada.\n",
        "4.  (Em um cenário real) Poderia armazenar esta métrica em outro banco de dados, enviar um email de notificação, etc.\n",
        "\n",
        "**Script Python com PySpark (para execução periódica):**"
      ],
      "metadata": {
        "id": "23h4CD1FzR6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "def calcular_vendas_dia_anterior():\n",
        "    \"\"\"Calcula o total de vendas do dia anterior do SQL Server.\"\"\"\n",
        "\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"CalculoVendasDiaAnterior\") \\\n",
        "        .config(\"spark.jars\", \"caminho/para/sqljdbc_auth.jar,caminho/para/mssql-jdbc-12.4.0.jre11.jar\") \\ # Ajuste o caminho dos drivers JDBC\n",
        "        .getOrCreate()\n",
        "\n",
        "    jdbcUrl = \"jdbc:sqlserver://servidorSQLServer:1433;databaseName=nomeDoBancoDeDados;integratedSecurity=true;\" # Ajuste a URL\n",
        "    connectionProperties = {\n",
        "        \"user\": \"seu_usuario_sqlserver\", # Ajuste as credenciais se necessário\n",
        "        \"password\": \"sua_senha_sqlserver\",\n",
        "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
        "    }\n",
        "\n",
        "    # Calcular a data de ontem no formato 'YYYY-MM-DD' para a consulta SQL\n",
        "    data_ontem = (datetime.date.today() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "\n",
        "    # Subconsulta SQL para selecionar vendas do dia anterior\n",
        "    subquery_vendas_ontem = f\"\"\"\n",
        "        (SELECT Quantidade, Preco_Unitario FROM Vendas WHERE CONVERT(DATE, Data_Venda) = '{data_ontem}') AS vendas_ontem\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        df_vendas_ontem = spark.read.jdbc(url=jdbcUrl, table=subquery_vendas_ontem, properties=connectionProperties)\n",
        "\n",
        "        # Calcular o total de vendas do dia anterior\n",
        "        total_vendas_ontem_df = df_vendas_ontem.agg(sum(df_vendas_ontem[\"Quantidade\"] * df_vendas_ontem[\"Preco_Unitario\"]).alias(\"Total_Vendas_Ontem\"))\n",
        "        total_vendas_ontem_row = total_vendas_ontem_df.collect()[0]\n",
        "        total_vendas_ontem = total_vendas_ontem_row[\"Total_Vendas_Ontem\"]\n",
        "\n",
        "        print(f\"Total de vendas de {data_ontem}: R$ {total_vendas_ontem:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao calcular vendas do dia anterior: {e}\")\n",
        "\n",
        "    finally:\n",
        "        spark.stop()\n",
        "\n",
        "# Executar a função para calcular as vendas do dia anterior\n",
        "calcular_vendas_dia_anterior()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "NXaojpCozR6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Passos e Explicações:**\n",
        "\n",
        "1.  **Importar bibliotecas:** `datetime` para manipular datas, `SparkSession` e `sum` do PySpark.\n",
        "2.  **Função `calcular_vendas_dia_anterior()`:** Encapsula toda a lógica.\n",
        "3.  **Inicialização SparkSession e configurações JDBC:**  Similar aos exemplos anteriores.\n",
        "4.  **Cálculo da data de ontem:** Usamos `datetime` para obter a data de ontem e formatá-la no formato 'YYYY-MM-DD' para usar na cláusula `WHERE` da consulta SQL.\n",
        "5.  **Subconsulta SQL:** Criamos uma subconsulta SQL formatada (f-string) para selecionar apenas as linhas da tabela `Vendas` onde a `Data_Venda` corresponde ao dia anterior. `CONVERT(DATE, Data_Venda)` é usado para comparar apenas a parte da data, ignorando a hora (dependendo do tipo de dado da sua coluna `Data_Venda`).\n",
        "6.  **Ler dados com `spark.read.jdbc()`:** Lemos os dados filtrados do dia anterior para um DataFrame.\n",
        "7.  **Calcular total de vendas:** Usamos `df_vendas_ontem.agg(sum(...))` para calcular a soma do produto de `Quantidade` e `Preco_Unitario` (total de vendas) do DataFrame.  `alias(\"Total_Vendas_Ontem\")` renomeia a coluna resultante.\n",
        "8.  **Obter resultado:** `total_vendas_ontem_df.collect()[0]` executa a ação `collect()` para trazer o resultado (que é uma única linha e coluna neste caso) para o Driver e `.collect()[0]` pega a primeira (e única) linha. `total_vendas_ontem_row[\"Total_Vendas_Ontem\"]` acessa o valor da coluna `Total_Vendas_Ontem` nessa linha.\n",
        "9.  **Imprimir resultado:** Imprimimos a métrica calculada formatada.\n",
        "10. **Tratamento de erros (`try...except`) e finalização (`finally spark.stop()`):**  Adicionamos tratamento de erros básico e garantimos que a SparkSession seja sempre encerrada, mesmo em caso de erro.\n",
        "11. **Executar a função:** Chamamos `calcular_vendas_dia_anterior()` para rodar o cálculo.\n",
        "\n",
        "**Agendamento para execução periódica:**\n",
        "\n",
        "Para executar este script periodicamente (por exemplo, todos os dias), você pode usar ferramentas de agendamento de tarefas do sistema operacional ou ferramentas de orquestração mais robustas:\n",
        "\n",
        "*   **Agendador de tarefas do sistema operacional (Windows Task Scheduler, cron no Linux/macOS):**  Para tarefas simples, pode agendar o script Python para rodar em um horário específico diariamente.\n",
        "*   **Apache Airflow, Prefect, etc.:** Para pipelines de dados mais complexos e agendamento robusto, ferramentas de orquestração como Airflow ou Prefect são mais adequadas. Elas oferecem funcionalidades como monitoramento, tratamento de dependências entre tarefas, retentativas em caso de falha, etc.\n",
        "\n",
        "**Parte 4: Alternativas ao PySpark para Python e SQL Server**\n",
        "\n",
        "Embora PySpark seja excelente para processamento de grandes dados, para tarefas mais simples ou quando não há necessidade de computação distribuída, você pode considerar outras bibliotecas Python para interagir com SQL Server:\n",
        "\n",
        "*   **`pyodbc`:** Uma das bibliotecas mais populares para conectar Python a bancos de dados ODBC, incluindo SQL Server.  É mais leve que PySpark e geralmente mais fácil de configurar para consultas simples."
      ],
      "metadata": {
        "id": "uh8Ao_ADzR6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyodbc\n",
        "\n",
        "    def calcular_vendas_dia_anterior_pyodbc():\n",
        "        \"\"\"Calcula vendas do dia anterior usando pyodbc.\"\"\"\n",
        "        try:\n",
        "            conexao = pyodbc.connect(\n",
        "                'DRIVER={SQL Server};'\n",
        "                'SERVER=servidorSQLServer;'\n",
        "                'DATABASE=nomeDoBancoDeDados;'\n",
        "                'UID=seu_usuario_sqlserver;'\n",
        "                'PWD=sua_senha_sqlserver;' # Ou usar autenticação Windows: 'Trusted_Connection=yes;'\n",
        "            )\n",
        "            cursor = conexao.cursor()\n",
        "\n",
        "            data_ontem = (datetime.date.today() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "            query_sql = f\"\"\"\n",
        "                SELECT SUM(Quantidade * Preco_Unitario) AS Total_Vendas_Ontem\n",
        "                FROM Vendas\n",
        "                WHERE CONVERT(DATE, Data_Venda) = '{data_ontem}'\n",
        "            \"\"\"\n",
        "            cursor.execute(query_sql)\n",
        "            resultado = cursor.fetchone()\n",
        "            total_vendas_ontem = resultado[0] if resultado[0] else 0\n",
        "\n",
        "            print(f\"Total de vendas de {data_ontem}: R$ {total_vendas_ontem:.2f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao calcular vendas do dia anterior com pyodbc: {e}\")\n",
        "\n",
        "        finally:\n",
        "            if conexao:\n",
        "                conexao.close()\n",
        "\n",
        "    calcular_vendas_dia_anterior_pyodbc()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "apxesKHDzR6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **`SQLAlchemy`:** Uma biblioteca ORM (Object-Relational Mapper) poderosa que oferece uma forma mais abstrata de interagir com bancos de dados. Permite definir modelos de dados em Python e realizar operações de banco de dados de forma mais orientada a objetos, além de suportar SQL \"raw\". É mais complexo que `pyodbc` para consultas simples, mas oferece mais flexibilidade para aplicações maiores e complexas.\n",
        "\n",
        "**Quando usar PySpark vs. Alternativas?**\n",
        "\n",
        "*   **PySpark:** Ideal para:\n",
        "    *   Grandes volumes de dados (Big Data) que não cabem na memória de uma única máquina.\n",
        "    *   Processamento distribuído e escalável.\n",
        "    *   Tarefas de análise de dados complexas, transformações, agregações em larga escala.\n",
        "    *   Integração com outras funcionalidades do Spark (MLlib para Machine Learning, Spark Streaming para processamento em tempo real).\n",
        "\n",
        "*   **`pyodbc`, `SQLAlchemy`:**  Mais adequados para:\n",
        "    *   Volumes de dados menores ou que cabem na memória da máquina.\n",
        "    *   Tarefas mais simples de consulta, inserção, atualização de dados.\n",
        "    *   Aplicações que não exigem processamento distribuído.\n",
        "    *   Desenvolvimento mais rápido e configuração mais simples para casos de uso menores.\n",
        "\n",
        "**Conclusão:**\n",
        "\n",
        "Este material detalhou como consultar o SQL Server utilizando SQL padrão e Python com PySpark.  Demonstramos exemplos práticos, incluindo o cálculo periódico de uma métrica, e mencionamos alternativas como `pyodbc` para cenários onde o Spark pode ser excessivo. A escolha da ferramenta dependerá do volume de dados, complexidade da tarefa e requisitos de escalabilidade do seu projeto."
      ],
      "metadata": {
        "id": "xjB-uJDnzR6E"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}